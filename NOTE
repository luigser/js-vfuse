Nodejs version (it works for browser but nodejs)
v12.16.3
PASS TO v14

/* RUN IPFS CLUSTER */
ipfs-cluster-service --loglevel info,cluster:debug,pintracker:debug daemon

USE ipfs-cluster_service for config cluster


TO START WEBRTC SIGNAL SERVER
node node_modules\libp2p-webrtc-star\src\sig-server\bin.js --port=2000


npm install ipfs-swarm-key-gen -g

//Profile ids
//WITH NODE API
12D3KooWHAe9ih2WHHsttbzyjfvWqCU9xXroiS8rb2BfqA495EdL //ALLINONE
12D3KooWCDmRPNZvne44L2ARDbrQB7X89c8g7XHjJqmPPUvA5vCr //ALLINONE
12D3KooWLGqTJsVQGMNrnLbRCGqPhHJrG9BLqA4xYLdWr3kCQfFq //SURFACE
//REGULAR
//Chrome
//ALLINONE
12D3KooWSz8xCReetryX7vpE7VNDDAxWtyh1pe6pWZ33nyoQzo8D
//SURFACE
12D3KooWEXq1trVMayJaFjnutSciyn3HgDrTu3NrKWVVTNpRo5Xa
//Firefox
//ALLINONE
12D3KooWEyShEo5ycTjet7AuJGFUWBryL9b48FhJVsRUZ4v61jE6
//SURFACE
12D3KooWJwkjUvWWu9vdzyE1d3CtkzXsmGDXaM8LXuCq6LjunWak

//Domande e note

Il workflow manager deve usare una factory per il runtime
Ogni runtime deve implementare il meccanismo di parsing per i jobs e dati

1) utilizzare un oggetto "VFuse.NOME_FUNZIONE"
    L'idea è passare da python a javascript per recuperare dati da IPFS/HTTP utilizzando funzioni scritte in Python ma implementate in javascript
        ESEMPIO:
        def map(input)
           ...
           print(output)
        def reduce(input)
           ....
           print(output)

        input = 'ipfs/HASH' //l'input potrebbe essere definito dall'esterno con un input box e qualcosa del genere
        for chank in range(input_size)
           data_chunk = VFuse.getBytes(input, chank, 1024)
           VFuse.createJob(map, data_chunk, [])
           VFuse.createJob(reduce, data_chunk, [map])


    - esiste un modo per mappare funzioni da un liguaggio ad un altro( design pattern, DSL o qualcosa del genere?)
        - VFUSE.getData(url, return_type) //quando non si vogliono definire i dati direttamente nel codice; return_type devev essere gestito dal manager


//TODO note

Quando si crea un nuovo repository, con un nuovo id, si ottiene un nuovo peer id

Lettura di file
Su browser è possibile utilizzare solo HTTP E WEB SOCKET
per leggere un parte di un file si può usare l'header range che deve essere supportato dal server
var xmlhttp=new XMLHttpRequest();
xmlhttp.open("GET","data.dat",false);
xmlhttp.setRequestHeader("Range", "bytes=100-200");
xmlhttp.send();
console.info(xmlhttp); //--> returns only the partial content

//TODO
Indicizzare i workflows in locale
    -quando un workflow è molto grosso capita che la filtering impieghi molto tempo per ottenere i risultati

//TODO NEW
    - TIMEOUT sull'esecuzione del job ( evitare overload) [X]
    - reward con blockchain *** NEXT VERSION
    - meccanismo per la REPUTATION *** NEXT VERSION
    - meccanismo per il CHECKING dei risultati (possibile accumulare i risultati di più esecuzioni e confrontarli) *** NEXT VERSION
    - limitare il numero di workflow su cui lavorare in un determinato momento[X]
    - marcare i job di OUTPUT (invorsimile in alcuni casi recuperare l'output dal DAG)[ X - Restiuire i nodi senza figli]
    - prevedere un meccanismo per registrare le funzioni senza includere il codice in ogni job ma recuperandolo dal workflow
    - definire un meccanismo che consenta ai job di ricevere uno stream senza diventare COMPLETED *** ENDLESS JOB *** IMPORTANTE PER COMPLETEZZA DEL MODELLO DI PROGRAMMAZIONE[*X]
    - pagine dei workflow in esecuzione con la possibilità di salvare il workflow nel proprio spazio privato *** IMPORTANTE
    - salvare le preferenze *** IMPORTANTE
    - PYTHON NODEJS WORKER *** IN SEGUITO MA IMPORTANTE

    - Completare e testare gli endless jobs
    - Completare e testare la register function
    - Aggiungere gli snippert per vfuse

METTERE SU IL SISTEMA SU 172.16.15.178
user: isis pwd: clgvittorio
