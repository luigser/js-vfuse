Nodejs version (it works for browser but nodejs)
v12.16.3
PASS TO v14

/* RUN IPFS CLUSTER */
ipfs-cluster-service --loglevel info,cluster:debug,pintracker:debug daemon

USE ipfs-cluster_service for config cluster


TO START WEBRTC SIGNAL SERVER
node node_modules\libp2p-webrtc-star\src\sig-server\bin.js --port=2000


npm install ipfs-swarm-key-gen -g

//Profile ids
//WITH NODE API
12D3KooWHAe9ih2WHHsttbzyjfvWqCU9xXroiS8rb2BfqA495EdL //ALLINONE
12D3KooWCDmRPNZvne44L2ARDbrQB7X89c8g7XHjJqmPPUvA5vCr //ALLINONE
12D3KooWLGqTJsVQGMNrnLbRCGqPhHJrG9BLqA4xYLdWr3kCQfFq //SURFACE
//REGULAR
//Chrome
12D3KooWAFC525pEtNwBxSiunRainuwPyLvYyU5BKzX3HG4RPrHE
//Firefox
12D3KooWDDnwGGZcq6LzayYkjAYD5BUvU29h9QVVvFTKkgisssQE

//Domande e note

Il workflow manager deve usare una factory per il runtime
Ogni runtime deve implementare il meccanismo di parsing per i jobs e dati

1) utilizzare un oggetto "VFuse.NOME_FUNZIONE"
    L'idea è passare da python a javascript per recuperare dati da IPFS/HTTP utilizzando funzioni scritte in Python ma implementate in javascript
        ESEMPIO:
        def map(input)
           ...
           print(output)
        def reduce(input)
           ....
           print(output)

        input = 'ipfs/HASH' //l'input potrebbe essere definito dall'esterno con un input box e qualcosa del genere
        for chank in range(input_size)
           data_chunk = VFuse.getBytes(input, chank, 1024)
           VFuse.createJob(map, data_chunk, [])
           VFuse.createJob(reduce, data_chunk, [map])


    - esiste un modo per mappare funzioni da un liguaggio ad un altro( design pattern, DSL o qualcosa del genere?)
        - VFUSE.getData(url, return_type) //quando non si vogliono definire i dati direttamente nel codice; return_type devev essere gestito dal manager


input = [[2, 0], [0, 2], [4, 1], [2, 2]]
OR
input = VFuse.getData("http://IP/ipfs/CID", start, end, VFuse.ArrayString) // Worker -> factory -> getData per Python

def partion2()
   return VFuse.getData("http://IP/ipfs/CID", start, end, VFuse.ArrayString)

def partition(input)
   return 4 elementi [2, 0], [0, 2], [4, 1], [2, 2]

def task(data)
   data[0] + data[1]


def somma(data)
   somma = 0
   for data 1...data.length
      somma += data[i]

VFuse.addJobsInput(partition, input) //workflow -> /data/input 1,2,3,4 ->  [2, 0], [0, 2], [4, 1], [2, 2]  // corrisponde a un file di result
VFuse.addJobs(task,  [partition])
VFuse.addJobs(somma, [task])

Datalet che filtrano big data in parallelo


//JAVASCRIPT
let chunck = 1000
for(let i=0; i < input.length; i+=chunk){
   let chunck_lenght = 1000
   if(i + chunck > input.length)
      chunk_length = chunk - ((i + chunk) - input.lenght)
   VFuse.addJob()
}

MAP-REDUCE EXAMPLE

let input = "Contrary to popular belief, Lorem Ipsum is not simply random text.\n" +
        "It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old.\n" +
        "Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur,\n" +
        "from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source.\n" +
        " Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero,\n" +
        " written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum," +
        "\"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32."
function map(data){
   let tokens = []
   data.split(/\W+/).map(word => tokens.push({ word : word , count : 1 }))
   return tokens
}

function reduce(data){
   let reduced = new Map()
   data.map( entry => reduced.set(entry.word, reduced.has(entry.word) ? reduced.get(entry.word) + 1 : 1))
   return reduced
}

function combine(data){
   let result = new Map()
   for(let d of data)
      result.set(d.key, result.has(d.key) ? result.get(d.key) + d.value : d.value)
   return result
}

input = input.split("\n")
let reduced_results = []
for (let row in input){
   let mapped = await VFuse.addJob(map, [], input[row])
   let reduced = await VFuse.addJob(reduce, [mapped])
   reduced_results.push(mapped)
}
let result = await VFuse.addJob(combine, ['reduce'])//wait for all reduce results and call combine




//TODO note

Quando si crea un nuovo repository, con un nuovo id, si ottiene un nuovo peer id

Lettura di file
Su browser è possibile utilizzare solo HTTP E WEB SOCKET
per leggere un parte di un file si può usare l'header range che deve essere supportato dal server
var xmlhttp=new XMLHttpRequest();
xmlhttp.open("GET","data.dat",false);
xmlhttp.setRequestHeader("Range", "bytes=100-200");
xmlhttp.send();
console.info(xmlhttp); //--> returns only the partial content

//TODO
SI VA IN ERRORE QUANDO IL PROFILO GIà ESISTE E SI CERCA DI CREARNE UN ALTRO.
QUANDO SI SALVA IL RPOFILO QUESTO VIENE APPESO A QUELLO PRECEDENTE
L'ERRORE è DOCUTO ALLA SOVREAPPOSOZIONE NEL DAG DEI BLOCCHI

SALVARE IL DAG

DIPENDENZE COME ID DEL JOBS / NOME FUNZIONE PER INDICARE UNA DIPENDENZA TOTALE DAI RISULTATI DELLA STESSA //Done

SUBMIT WORKFLOW DA CODICE PER LA SOTTOMISSINE DEL WORKFLOW NELLA RETE

CHECK WORKFLOW PER OTTENERE INFO SULLO STATO DEL DAG ASSOCIATO

TASK COMPUTATI DA MOLTI NODI PER GARANTIRE LA VALIDITà


//TODO
SE IL TAB NON è ATTIVO IL SISTEMA SI FERMA ... RISOLVERE CON WORKER O QALCOSA DEL GENERE

//TODO
é nescessatio che il workflow continuo la sua esecuzione nella sua interezza anche quando il nodo che lo ha generato
non è online.
- Questo significa che bisogna condividere il workflow non i job
- bisogna fare gossip del workflow aggiornato
- bisogna gestire i risultati con i cid
- il creatore del workflow deve ottenere i risultati anche dopo il tempo limite (48h al momento ma configurabile)

POSSIBILE SOLUZIONE

1)
    - pubblico workflow -> { wid: id, cid: cid }
    - metto in /workflows/published/wid.json -> { wid: workflow, cid: cid }
    - aggiorno in profilo publishedWorkflows per tenere traccia dei mei workflow pubblicati
    - pubblico all READY jobs for all workflows in /workflows/published/*.json
2)ricevo un workflow
    - controllo se il workflows che ho ha un numero di jobs READY minore di quello che ricevo
        - controllo anche il numero di repliche per i results
    - se quello che ho ricevuto è più aggiornato -> salvo in file /workflows/published/wid.json
3)esecuzione
    - seleziono casualmente un wid da /workflows/published/*.json
    - eseguo i jobs READY ma prima recupero l'input dal CID
    - aggiorno il workflow selezinato
    - pubblico il workflow aggiornato
